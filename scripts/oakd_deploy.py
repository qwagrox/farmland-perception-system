#!/usr/bin/env python3
"""
OAK-DÈÉ®ÁΩ≤ËÑöÊú¨
ÂäüËÉΩÔºöÂú®OAK-DÁõ∏Êú∫‰∏äËøêË°åË°å‰∫∫ÂíåÊãñÊãâÊú∫Ê£ÄÊµã
"""

import cv2
import depthai as dai
import numpy as np
import time
import argparse
from pathlib import Path

# Á±ªÂà´ÈÖçÁΩÆ
LABELS = ['person', 'tractor']
COLORS = {
    'person': (0, 255, 0),    # ÁªøËâ≤
    'tractor': (0, 0, 255)    # Á∫¢Ëâ≤
}

# ÂÆâÂÖ®Ë∑ùÁ¶ªÈÖçÁΩÆ
SAFE_DISTANCE = 5.0      # Á¥ßÊÄ•ÂÅúÊ≠¢Ë∑ùÁ¶ªÔºàÁ±≥Ôºâ
WARNING_DISTANCE = 10.0  # Ë≠¶ÂëäË∑ùÁ¶ªÔºàÁ±≥Ôºâ

class OAKDDetector:
    """OAK-DÊ£ÄÊµãÂô®Á±ª"""
    
    def __init__(self, blob_path, conf_threshold=0.5, iou_threshold=0.5):
        """
        ÂàùÂßãÂåñÊ£ÄÊµãÂô®
        
        Args:
            blob_path: BlobÊ®°ÂûãË∑ØÂæÑ
            conf_threshold: ÁΩÆ‰ø°Â∫¶ÈòàÂÄº
            iou_threshold: IOUÈòàÂÄº
        """
        self.blob_path = blob_path
        self.conf_threshold = conf_threshold
        self.iou_threshold = iou_threshold
        self.pipeline = None
        self.device = None
        
    def create_pipeline(self):
        """ÂàõÂª∫DepthAI pipeline"""
        print("ÂàõÂª∫DepthAI pipeline...")
        
        pipeline = dai.Pipeline()
        
        # RGBÁõ∏Êú∫
        cam_rgb = pipeline.create(dai.node.ColorCamera)
        cam_rgb.setPreviewSize(416, 416)
        cam_rgb.setResolution(dai.ColorCameraProperties.SensorResolution.THE_1080_P)
        cam_rgb.setInterleaved(False)
        cam_rgb.setColorOrder(dai.ColorCameraProperties.ColorOrder.BGR)
        cam_rgb.setFps(30)
        
        # ÂçïÁõÆÁõ∏Êú∫ÔºàÁî®‰∫éÁ´ã‰ΩìÊ∑±Â∫¶Ôºâ
        mono_left = pipeline.create(dai.node.MonoCamera)
        mono_right = pipeline.create(dai.node.MonoCamera)
        
        mono_left.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)
        mono_right.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)
        mono_left.setBoardSocket(dai.CameraBoardSocket.LEFT)
        mono_right.setBoardSocket(dai.CameraBoardSocket.RIGHT)
        
        # Á´ã‰ΩìÊ∑±Â∫¶
        stereo = pipeline.create(dai.node.StereoDepth)
        stereo.setDefaultProfilePreset(dai.node.StereoDepth.PresetMode.HIGH_DENSITY)
        stereo.setLeftRightCheck(True)
        stereo.setSubpixel(True)
        stereo.setDepthAlign(dai.CameraBoardSocket.RGB)
        
        mono_left.out.link(stereo.left)
        mono_right.out.link(stereo.right)
        
        # Á•ûÁªèÁΩëÁªú
        detection_nn = pipeline.create(dai.node.YoloDetectionNetwork)
        detection_nn.setBlobPath(self.blob_path)
        detection_nn.setConfidenceThreshold(self.conf_threshold)
        detection_nn.setNumClasses(len(LABELS))
        detection_nn.setCoordinateSize(4)
        detection_nn.setAnchors([])
        detection_nn.setAnchorMasks({})
        detection_nn.setIouThreshold(self.iou_threshold)
        detection_nn.setNumInferenceThreads(2)
        detection_nn.input.setBlocking(False)
        
        cam_rgb.preview.link(detection_nn.input)
        
        # ËæìÂá∫
        xout_rgb = pipeline.create(dai.node.XLinkOut)
        xout_rgb.setStreamName("rgb")
        cam_rgb.preview.link(xout_rgb.input)
        
        xout_nn = pipeline.create(dai.node.XLinkOut)
        xout_nn.setStreamName("detections")
        detection_nn.out.link(xout_nn.input)
        
        xout_depth = pipeline.create(dai.node.XLinkOut)
        xout_depth.setStreamName("depth")
        stereo.depth.link(xout_depth.input)
        
        self.pipeline = pipeline
        print("‚úÖ PipelineÂàõÂª∫ÊàêÂäü")
        
        return pipeline
    
    def calculate_distance(self, depth_frame, bbox):
        """
        ËÆ°ÁÆóÁõÆÊ†áË∑ùÁ¶ª
        
        Args:
            depth_frame: Ê∑±Â∫¶Â∏ß
            bbox: ËæπÁïåÊ°Ü [x1, y1, x2, y2]
        
        Returns:
            distance: Ë∑ùÁ¶ªÔºàÁ±≥Ôºâ
        """
        x1, y1, x2, y2 = bbox
        
        # Ë∞ÉÊï¥ÂùêÊ†áÂà∞Ê∑±Â∫¶Â∏ßÂ∞∫ÂØ∏
        h, w = depth_frame.shape
        x1 = int(x1 * w / 416)
        y1 = int(y1 * h / 416)
        x2 = int(x2 * w / 416)
        y2 = int(y2 * h / 416)
        
        # Ëé∑Âèñ‰∏≠ÂøÉÂå∫Âüü
        center_x = (x1 + x2) // 2
        center_y = (y1 + y2) // 2
        
        # ÈááÊ†∑Âå∫Âüü
        roi_size = 5
        y_min = max(0, center_y - roi_size)
        y_max = min(h, center_y + roi_size)
        x_min = max(0, center_x - roi_size)
        x_max = min(w, center_x + roi_size)
        
        depth_roi = depth_frame[y_min:y_max, x_min:x_max]
        
        # ËøáÊª§Êó†ÊïàÊ∑±Â∫¶ÂÄº
        valid_depths = depth_roi[depth_roi > 0]
        
        if len(valid_depths) == 0:
            return -1
        
        # ‰ΩøÁî®‰∏≠‰ΩçÊï∞
        distance = np.median(valid_depths) / 1000.0  # ËΩ¨Êç¢‰∏∫Á±≥
        
        return distance
    
    def run(self, show_fps=True, save_video=False, output_path='output.avi'):
        """
        ËøêË°åÊ£ÄÊµã
        
        Args:
            show_fps: ÊòØÂê¶ÊòæÁ§∫FPS
            save_video: ÊòØÂê¶‰øùÂ≠òËßÜÈ¢ë
            output_path: ËßÜÈ¢ë‰øùÂ≠òË∑ØÂæÑ
        """
        if self.pipeline is None:
            self.create_pipeline()
        
        print("\nÊ≠£Âú®ËøûÊé•OAK-DÁõ∏Êú∫...")
        
        try:
            with dai.Device(self.pipeline) as device:
                print("‚úÖ OAK-DÁõ∏Êú∫Â∑≤ËøûÊé•")
                
                # Ëé∑ÂèñËæìÂá∫ÈòüÂàó
                q_rgb = device.getOutputQueue(name="rgb", maxSize=4, blocking=False)
                q_det = device.getOutputQueue(name="detections", maxSize=4, blocking=False)
                q_depth = device.getOutputQueue(name="depth", maxSize=4, blocking=False)
                
                # ËßÜÈ¢ëÂÜôÂÖ•Âô®
                video_writer = None
                if save_video:
                    fourcc = cv2.VideoWriter_fourcc(*'XVID')
                    video_writer = cv2.VideoWriter(output_path, fourcc, 30.0, (416, 416))
                
                frame_count = 0
                start_time = time.time()
                fps = 0
                
                print("\nÂºÄÂßãÊ£ÄÊµã...")
                print("Êåâ 'q' ÈÄÄÂá∫, 'p' ÊöÇÂÅú, 's' Êà™Âõæ")
                print("=" * 60)
                
                paused = False
                
                while True:
                    if not paused:
                        # Ëé∑ÂèñÂ∏ß
                        in_rgb = q_rgb.get()
                        in_depth = q_depth.get()
                        in_det = q_det.get()
                        
                        frame = in_rgb.getCvFrame()
                        depth_frame = in_depth.getFrame()
                        detections = in_det.detections
                        
                        # Â§ÑÁêÜÊ£ÄÊµãÁªìÊûú
                        closest_distance = float('inf')
                        closest_label = None
                        
                        for detection in detections:
                            # ËæπÁïåÊ°Ü
                            bbox = [
                                int(detection.xmin * 416),
                                int(detection.ymin * 416),
                                int(detection.xmax * 416),
                                int(detection.ymax * 416)
                            ]
                            
                            # Á±ªÂà´ÂíåÁΩÆ‰ø°Â∫¶
                            label_id = detection.label
                            if label_id >= len(LABELS):
                                continue
                            
                            label = LABELS[label_id]
                            confidence = detection.confidence
                            
                            # ËÆ°ÁÆóË∑ùÁ¶ª
                            distance = self.calculate_distance(depth_frame, bbox)
                            
                            if distance < 0:
                                distance_text = "N/A"
                            else:
                                distance_text = f"{distance:.2f}m"
                                
                                # ËÆ∞ÂΩïÊúÄËøëÁöÑÁõÆÊ†á
                                if distance < closest_distance:
                                    closest_distance = distance
                                    closest_label = label
                            
                            # ÁªòÂà∂ËæπÁïåÊ°Ü
                            color = COLORS.get(label, (255, 255, 255))
                            cv2.rectangle(frame, (bbox[0], bbox[1]), (bbox[2], bbox[3]), color, 2)
                            
                            # ÁªòÂà∂Ê†áÁ≠æ
                            text = f"{label}: {confidence:.2f} | {distance_text}"
                            text_size = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)[0]
                            cv2.rectangle(frame, (bbox[0], bbox[1] - text_size[1] - 10),
                                        (bbox[0] + text_size[0], bbox[1]), color, -1)
                            cv2.putText(frame, text, (bbox[0], bbox[1] - 5),
                                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)
                        
                        # ÂÆâÂÖ®Ë≠¶Âëä
                        if closest_distance < SAFE_DISTANCE:
                            warning_text = f"STOP! {closest_label} at {closest_distance:.2f}m"
                            cv2.putText(frame, warning_text, (10, 30),
                                       cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 3)
                            print(f"üõë Á¥ßÊÄ•ÂÅúÊ≠¢: {closest_label} Ë∑ùÁ¶ª {closest_distance:.2f}m")
                        elif closest_distance < WARNING_DISTANCE:
                            warning_text = f"SLOW! {closest_label} at {closest_distance:.2f}m"
                            cv2.putText(frame, warning_text, (10, 30),
                                       cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 165, 255), 3)
                            print(f"‚ö†Ô∏è  ÂáèÈÄü: {closest_label} Ë∑ùÁ¶ª {closest_distance:.2f}m")
                        
                        # ÊòæÁ§∫FPS
                        if show_fps:
                            frame_count += 1
                            if frame_count % 30 == 0:
                                fps = frame_count / (time.time() - start_time)
                            
                            cv2.putText(frame, f"FPS: {fps:.1f}", (10, frame.shape[0] - 10),
                                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
                        
                        # ‰øùÂ≠òËßÜÈ¢ë
                        if video_writer is not None:
                            video_writer.write(frame)
                        
                        # ÊòæÁ§∫
                        cv2.imshow("OAK-D Detection", frame)
                    
                    # ÈîÆÁõòÊéßÂà∂
                    key = cv2.waitKey(1) & 0xFF
                    if key == ord('q'):
                        break
                    elif key == ord('p'):
                        paused = not paused
                        print("ÊöÇÂÅú" if paused else "ÁªßÁª≠")
                    elif key == ord('s'):
                        screenshot_path = f"screenshot_{int(time.time())}.jpg"
                        cv2.imwrite(screenshot_path, frame)
                        print(f"Êà™ÂõæÂ∑≤‰øùÂ≠ò: {screenshot_path}")
                
                # Ê∏ÖÁêÜ
                if video_writer is not None:
                    video_writer.release()
                
                cv2.destroyAllWindows()
                print("\nÊ£ÄÊµãÂ∑≤ÂÅúÊ≠¢")
                
        except Exception as e:
            print(f"\n‚ùå ÈîôËØØ: {str(e)}")
            print("\nËØ∑Ê£ÄÊü•:")
            print("1. OAK-DÁõ∏Êú∫ÊòØÂê¶Ê≠£Á°ÆËøûÊé•")
            print("2. BlobÊñá‰ª∂ÊòØÂê¶Â≠òÂú®‰∏îÊúâÊïà")
            print("3. ÊòØÂê¶Â∑≤ÂÆâË£Ödepthai: pip install depthai")

def main():
    """‰∏ªÂáΩÊï∞"""
    parser = argparse.ArgumentParser(description='OAK-DË°å‰∫∫ÂíåÊãñÊãâÊú∫Ê£ÄÊµã')
    parser.add_argument('--blob', type=str, default='models/best.blob',
                       help='BlobÊ®°ÂûãË∑ØÂæÑ')
    parser.add_argument('--conf', type=float, default=0.5,
                       help='ÁΩÆ‰ø°Â∫¶ÈòàÂÄº')
    parser.add_argument('--iou', type=float, default=0.5,
                       help='IOUÈòàÂÄº')
    parser.add_argument('--save', action='store_true',
                       help='‰øùÂ≠òÊ£ÄÊµãËßÜÈ¢ë')
    parser.add_argument('--output', type=str, default='output.avi',
                       help='ËæìÂá∫ËßÜÈ¢ëË∑ØÂæÑ')
    
    args = parser.parse_args()
    
    # Ê£ÄÊü•blobÊñá‰ª∂
    if not Path(args.blob).exists():
        print(f"‚ùå ÈîôËØØ: BlobÊñá‰ª∂‰∏çÂ≠òÂú®: {args.blob}")
        print("ËØ∑ÂÖàËøêË°å convert_to_blob.py ËΩ¨Êç¢Ê®°Âûã")
        return
    
    print("=" * 60)
    print("OAK-D Ë°å‰∫∫‰∏éÊãñÊãâÊú∫Ê£ÄÊµãÁ≥ªÁªü")
    print("=" * 60)
    print(f"Ê®°Âûã: {args.blob}")
    print(f"ÁΩÆ‰ø°Â∫¶ÈòàÂÄº: {args.conf}")
    print(f"IOUÈòàÂÄº: {args.iou}")
    print(f"ÂÆâÂÖ®Ë∑ùÁ¶ª: {SAFE_DISTANCE}m")
    print(f"Ë≠¶ÂëäË∑ùÁ¶ª: {WARNING_DISTANCE}m")
    print("=" * 60)
    
    # ÂàõÂª∫Ê£ÄÊµãÂô®
    detector = OAKDDetector(
        blob_path=args.blob,
        conf_threshold=args.conf,
        iou_threshold=args.iou
    )
    
    # ËøêË°åÊ£ÄÊµã
    detector.run(show_fps=True, save_video=args.save, output_path=args.output)

if __name__ == "__main__":
    main()

